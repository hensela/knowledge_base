Time-series $(y_t)_{t \in \mathbb{N}}$


## [Time-series components](https://otexts.com/fpp2/components.html) and [patterns](https://otexts.com/fpp2/tspatterns.html)

**Additive decomposition:**  $ğ‘¦_ğ‘¡ = ğ‘†_ğ‘¡ + ğ‘‡_ğ‘¡ + ğ‘…_ğ‘¡$
 
Appropriate if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series.

**Multiplicative decomposition:**  $ğ‘¦_ğ‘¡ = ğ‘†_ğ‘¡ * ğ‘‡_ğ‘¡ * ğ‘…_ğ‘¡$
 
Appropriate when the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series. 
Can be log transformed to get an additive decomposition:

$$ğ‘¦_ğ‘¡ = ğ‘†_ğ‘¡ * ğ‘‡_ğ‘¡ * ğ‘…_ğ‘¡ \Leftrightarrow ğ‘™ğ‘œğ‘”(ğ‘¦_ğ‘¡) = log(ğ‘†_ğ‘¡) + log(ğ‘‡_ğ‘¡) + log(ğ‘…_ğ‘¡)$$
 
- $ğ‘†_ğ‘¡$: seasonal component
- $ğ‘‡_ğ‘¡$: trend-cycle component
- $ğ‘…_ğ‘¡$: remainder component


## [Stationarity](https://otexts.com/fpp2/stationarity.html)

<details>
<summary><b>Details</b></summary>

A stationary time series is one whose properties do not depend on the time at which the series is observed:
- constant mean ($\leftrightarrow$ no trend)
- constant standard deviation
- no seasonality

A white noise series is stationary (key property: it's not predictable):
- mean = 0
- constant standard deviation
- correlation between lags is zero

A time-series with cyclic behaviour (but no trend or seasonality) is stationary because the cycles are not of a fixed length, so before we observe the series we cannot be sure where the peaks and troughs of the cycles will be.

**Log-transformation** can help stabilize the variance of a time-series.

**Differencing** can help stabilise the mean of a time-series and eliminate/reduce trend and seasonality. 
The differenced series is the change between consecutive observations $y_t' = y_t - y_{t-1}$
- This can be applied multiple times, or there's also seasonal differencing $y_t' = y_t - y_{t-m}$ for $m \geq 1$.
- If differencing is used, it's important that the differences are interpretable.

How to test for stationarity:
- visually
- global vs. local tests
- ACF: for a stationary time-series, the ACF will drop to zero relatively quickly
- statistical hypothesis tests for stationarity to more objectively determine if differencing is required, such as the *unit root test* or *augmented Dickey-Fuller test*
</details>


## [ACF](https://otexts.com/fpp2/autocorrelation.html) and [PACF](https://otexts.com/fpp2/non-seasonal-arima.html)

<details>
<summary><b>ACF</b></summary>

Complete auto-correlation function, giving auto-correlation values of a series with its lagged values. 
Describes how much the present value of a series is related with its past values. 
A time-series can have components like trend, seasonality, cyclic and residual. 
ACF considers all these components when finding correlations.

--> Used to find the order of the moving average (MA) process
</details>

<details>
<summary><b>PACF</b></summary>

Partial auto-correlation function. 
Instead of finding correlation of present values with all lags like ACF, it finds the correlation of the residual (i.e. what remains after removing effects already explained by earlier lags) with the next lag value. 
Essentially at each time $t$ it calculates the "pure" correlations between $y_t$ and $y_{t-k}$ (for $ğ‘˜ \geq 1$), 
removing any "indirect" effects of the type $ğ‘¦_{ğ‘¡âˆ’ğ‘˜} \rightarrow ğ‘¦_{ğ‘¡âˆ’ğ‘˜+1} \rightarrow ... \rightarrow ğ‘¦_ğ‘¡$ and only considering the "direct" effect $ğ‘¦_{ğ‘¡âˆ’ğ‘˜} \rightarrow ğ‘¦_ğ‘¡$.

The first partial autocorrelation is identical to the first autocorrelation. 
The $ğ‘˜$-th partial autocorrelation coefficient is equal to the estimate of $\phi_ğ‘˜$ in an $AR(ğ‘˜)$ model.

--> Used to find the order of the auto-regressive (AR) process
</details>


## [Autoregressive model (AR)](https://otexts.com/fpp2/AR.html)

<details>
<summary><b>Details</b></summary>

An autoregressive model $AR(ğ‘)$ of order $ğ‘$ can be written as:

$$ğ‘¦_ğ‘¡ = ğ‘ + \phi_1 ğ‘¦_{t-1} + \phi_2 ğ‘¦_{ğ‘¡âˆ’2} + ... + \phi_ğ‘ ğ‘¦_{ğ‘¡âˆ’ğ‘} + \epsilon_ğ‘¡$$
 
where $\epsilon_ğ‘¡$ is white noise. 
Essentially a linear regression with lagged values of the time-series.

Requires the time-series to be stationary.
</details>


## [Moving average model (MA)](https://otexts.com/fpp2/MA.html)

<details>
<summary><b>Details</b></summary>

Rather than using past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model. 
A moving average model $MA(ğ‘)$ of order $ğ‘$ can be written as:

$$ğ‘¦_ğ‘¡ = ğ‘ + \epsilon_ğ‘¡ + \theta_1 \epsilon_{ğ‘¡âˆ’1} + \theta_2 \epsilon_{ğ‘¡âˆ’2} + ... + \theta_ğ‘ \epsilon_{ğ‘¡âˆ’ğ‘}$$
 
where $\epsilon_ğ‘¡$ is white noise. 
Note that we don't actually observe the values $\epsilon_ğ‘¡$, so it's not really a regression in the usual sense. 
Each value of $y_t$ can be thought of as a weighted moving average of the past $ğ‘$ forecast errors.

It's possible to write any stationary $AR(ğ‘)$ model as a $MA(\infty)$ model. 
For example for an $AR(1)$ model:

$`\begin{aligned}
ğ‘¦_ğ‘¡ &= \phi_1 ğ‘¦_{ğ‘¡âˆ’1} + \epsilon_ğ‘¡ \\
    &= \phi_1 ( \phi_1 ğ‘¦_{ğ‘¡âˆ’1} + \epsilon_{ğ‘¡âˆ’1} ) + \epsilon_t \\
    &= \phi_1^2 ğ‘¦_{ğ‘¡âˆ’1} + \phi_1 \epsilon_{ğ‘¡âˆ’1} + \epsilon_ğ‘¡ ... \\
    &= \epsilon_ğ‘¡ + \phi_1 \epsilon_{ğ‘¡âˆ’1} + \phi_1^2 \epsilon_{ğ‘¡âˆ’2} + \phi_1^3 \epsilon_{ğ‘¡âˆ’3} + ... 
\end{aligned}`$
 
The reverse holds under some constraints on the MA parameters, in which case the MA model is called **invertible**.
</details>
