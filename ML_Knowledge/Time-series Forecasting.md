Time-series $(y_t)_{t \in \mathbb{N}}$


## [Time-series components](https://otexts.com/fpp2/components.html) and [patterns](https://otexts.com/fpp2/tspatterns.html)

**Additive decomposition:**  $ğ‘¦_ğ‘¡ = ğ‘†_ğ‘¡ + ğ‘‡_ğ‘¡ + ğ‘…_ğ‘¡$
 
Appropriate if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series.

**Multiplicative decomposition:**  $ğ‘¦_ğ‘¡ = ğ‘†_ğ‘¡ * ğ‘‡_ğ‘¡ * ğ‘…_ğ‘¡$
 
Appropriate when the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series. 
Can be log transformed to get an additive decomposition:

$$ğ‘¦_ğ‘¡ = ğ‘†_ğ‘¡ * ğ‘‡_ğ‘¡ * ğ‘…_ğ‘¡ \Leftrightarrow ğ‘™ğ‘œğ‘”(ğ‘¦_ğ‘¡) = log(ğ‘†_ğ‘¡) + log(ğ‘‡_ğ‘¡) + log(ğ‘…_ğ‘¡)$$
 
- $ğ‘†_ğ‘¡$: seasonal component
- $ğ‘‡_ğ‘¡$: trend-cycle component
- $ğ‘…_ğ‘¡$: remainder component


## [Stationarity](https://otexts.com/fpp2/stationarity.html)

<details>
<summary><b>Details</b></summary>

A stationary time series is one whose properties do not depend on the time at which the series is observed:
- constant mean ($\leftrightarrow$ no trend)
- constant standard deviation
- no seasonality

A white noise series is stationary (key property: it's not predictable):
- mean = 0
- constant standard deviation
- correlation between lags is zero

A time-series with cyclic behaviour (but no trend or seasonality) is stationary because the cycles are not of a fixed length, so before we observe the series we cannot be sure where the peaks and troughs of the cycles will be.

**Log-transformation** can help stabilize the variance of a time-series.

**Differencing** can help stabilise the mean of a time-series and eliminate/reduce trend and seasonality. 
The differenced series is the change between consecutive observations $y_t' = y_t - y_{t-1}$
- This can be applied multiple times, or there's also seasonal differencing $y_t' = y_t - y_{t-m}$ for $m \geq 1$.
- If differencing is used, it's important that the differences are interpretable.

How to test for stationarity:
- visually
- global vs. local tests
- ACF: for a stationary time-series, the ACF will drop to zero relatively quickly
- statistical hypothesis tests for stationarity to more objectively determine if differencing is required, such as the *unit root test* or *augmented Dickey-Fuller test*
</details>


## [ACF](https://otexts.com/fpp2/autocorrelation.html) and [PACF](https://otexts.com/fpp2/non-seasonal-arima.html)

<details>
<summary><b>ACF</b></summary>

Complete auto-correlation function, giving auto-correlation values of a series with its lagged values. 
Describes how much the present value of a series is related with its past values. 
A time-series can have components like trend, seasonality, cyclic and residual. 
ACF considers all these components when finding correlations.

--> Used to find the order of the moving average (MA) process
</details>

<details>
<summary><b>PACF</b></summary>

Partial auto-correlation function. 
Instead of finding correlation of present values with all lags like ACF, it finds the correlation of the residual (i.e. what remains after removing effects already explained by earlier lags) with the next lag value. 
Essentially at each time $t$ it calculates the "pure" correlations between $y_t$ and $y_{t-k}$ (for $ğ‘˜ \geq 1$), 
removing any "indirect" effects of the type $ğ‘¦_{ğ‘¡âˆ’ğ‘˜} \rightarrow ğ‘¦_{ğ‘¡âˆ’ğ‘˜+1} \rightarrow ... \rightarrow ğ‘¦_ğ‘¡$ and only considering the "direct" effect $ğ‘¦_{ğ‘¡âˆ’ğ‘˜} \rightarrow ğ‘¦_ğ‘¡$.

The first partial autocorrelation is identical to the first autocorrelation. 
The $ğ‘˜$-th partial autocorrelation coefficient is equal to the estimate of $\phi_ğ‘˜$ in an $AR(ğ‘˜)$ model.

--> Used to find the order of the auto-regressive (AR) process
</details>
