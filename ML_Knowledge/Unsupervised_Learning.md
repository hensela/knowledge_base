# Principal Component Analysis (PCA)
Basically, PCA relies on the singular value decomposition of the matrix of data points, which is of the form  $洧녦=洧녣洧냥洧녤^洧녢$, where both U and V are orthogonal matrices and the first m columns of V are the first m principal components.

## Idea of the algorithm:

1. Center the data points by subtracting the mean
2. Do a linear regression using sum of squared errors (SSE) as loss function
3. Note that SSE can be seen in two equivalent ways: 
One way is to minimize the sum of squared distances of the data points to the fitted line, let's call this  $洧녩_洧녰$ for a data point  $洧논_洧녰$
(that's already been shifted by the mean of data points). 
Let  $洧녫_洧녰=||洧논_洧녰||$ be the distance of  $洧논_洧녰$ to the origin. 
Then, by Pythagoras, the value $洧녪_洧녰$ that solves the equation  $洧녫_洧녰^2=洧녩_洧녰^2+洧녪_洧녰^2$ is the distance of the projection of  $洧논_洧녰$ onto the fitted line to the origin. 
One observes that minimizing the sum of squared errors  $\sum_i{洧녩_洧녰^2}$ corresponds to maximizing the sum $\sum_i{b_洧녰^2}$. 
The second sum is interpreted as the variance of the data along the fitted line. 
Hence the fitted line, corresponds to the direction along which the variance of the data is the largest, which is called the first principal component.
4. To find the next principal component, the restriction of it having to be orthogonal to all the previous ones is added.